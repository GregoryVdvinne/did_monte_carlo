---
title: "Exploring Modern Difference-in-Differences Techniques with A Monte Carlo Simulation"
author: "Gregory Vander Vinne"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    toc: true
    toc_float: false
---

```{r setup, include=TRUE, message=FALSE, warning=FALSE}

### To do:
  # Look into using map_dfr to bind multiple runs of the sim together 
  # Make own plotting function. Would include using summarise to average across sim runs 

#Clear memory
rm(list = ls(all=T))


#Load packages and install if not installed
if (!require("pacman")) install.packages("pacman")
pacman::p_load(
  
  tidyverse,  # Grammar for data + ggplot2
  did,        # Callaway and Sant'Anna
  did2s,      # Gardner
  data.table, # easy to make data tables
  knitr       # Print pretty tables
  
)

#Set some output options
knitr::opts_chunk$set(include = TRUE, warning = FALSE, message = FALSE, 
                      fig.width = 10, fig.height = 7)

```

```{r User-Defined Functions}

my_event_study <- function(data, xformla = NULL, true_effect) {
  rbind(
    event_study(
      yname = "revenue",  
      tname = "year",       
      idname = "id",        
      gname = "g",
      estimator = "TWFE",
      xformla = xformla,
      data = data
      ),
    event_study(
      yname = "revenue",  
      tname = "year",       
      idname = "id",        
      gname = "g",
      estimator = "did2s",
      xformla = xformla,
      data =data
      ),
    event_study(
      yname = "revenue",  
      tname = "year",       
      idname = "id",        
      gname = "g",
      estimator = "did",
      xformla = xformla,
      data = data
      )
  ) %>%
    left_join(true_effect, by = "term")
}


```

## Introduction

In my day job, I do a fair bit of work in the realm of causal inference. Typically, this involves looking at whether a particular policy resulted in improved outcomes for individuals who experienced the policy. There are many different ways of doing this. One of the most common methods, or groups of methods, is difference-in-differences (DiD). DiD methods have been advancing rapidly in recent years following the discovery of a number of issues with two-way fixed effects estimators (see Borusyak and Jaravel (2018), de Chaisemartin and D'Haultfoeuille (2020), Goodman-Bacon (2021), and Sun and Abraham (2021). See Roth (2023) for a synthesis of the literature).

Recently I have been exploring how different variations of difference-in-differences estimators that allow for staggered adoption behave under different circumstances. The {did2s} and {did} packages in R make this relatively easy to do with Monte Carlo simulations. While I have explored other estimators as well, in this post I wish to focus on two in particular. These two are the estimators proposed in Gardner (2021), and Callaway and Sant'Anna (2020). I will not be providing any analytical proofs or getting into any of the math behind how these to methods work. Rather, I will be illustrating with Monte Carlo simulations how these estimators behave (or misbehave) given different types of treatment effects and covariates. I will also be comparing them to the classic two-way fixed effects estimator. 

## Simulations

I find simulation-based explanations easier to follow when they are explained in terms of a hypothetical real world scenario and we give the variables realistic values and names. For this simulation, let us imagine that a large company with many locations wants to measure whether adoption of a particular technology improves sales. Some of this company's locations have adopted the technology, while others have not. Furthermore, the locations that have adopted the technology have adopted it at different times and have chosen to adopt it of their own volition, which means that the treatment is not randomly assigned. We shall explore how we might measure the effects of the technology on sales using the methods of Callaway and Sant'Anna (2021), Gardner (2021), and Two-Way Fixed Effects, hereafter referred to as TWFE.

### Homogeneous Treatment and Covariates with Fixed Effects

``` {r set parameters}
#Set number of units and periods
n.units <- 10000
n.periods <- 5

```

To begin, we have a balanced panel consisting of `r n.periods` periods (years) and `r n.units` units (store locations). Approximately 50%  of stores have not adopted the technology (i.e., they are untreated), while the other 50% have adopted the technology (i.e., they are treated at some point). Among these stores, approximately 1/4 adopted the technology in each of the last four years of our panel. In the first year, none of the stores have adopted the technology yet. For now, we will say that stores are treated in the first year that they adopt the technology and remain treated in all subsequent years. Additionally, there is one covariate, which is a dummy variable that indicates whether a store is located in an urban area or not. Approximately half of stores are in urban areas. Stores located in urban areas are twice as likely to adopt the technology as stores in non-urban areas; approximately 2/3 stores in urban areas adopt the technology, while 1/3 of stores in non-urban areas do. 

Lastly, we have revenue, which is determined as follows. For each store, revenue in each year is equal to 1 million, plus the effects of being located in an urban area and adopting the technology and an idiosyncratic error term drawn from a random normal distribution with a mean of zero and a standard deviation of 100 thousand. The ceteris paribus effect of being located in an urban area is an additional 50 thousand per year of revenue, while the ceteris paribus effect of the treatment is an additional 100 thousand per year of revenue. This effect is completely homogeneous. 

```{r Generate Data, echo=TRUE}

#First column
myData <- as.data.frame(rep(1:n.units, n.periods))
colnames(myData) <- "id"

#For reproducibility
set.seed(123)
#Add other columns
myData <- myData %>%
  arrange(by=id) %>% 
  cbind(year = rep(2019:(2019+n.periods-1),n.units)) %>% #Add periods
  group_by(id) %>%
  mutate(g = case_when(        #Assign to treatment groups
    id %in% (1:n.units/2) ~ 0, #50% untreated
    id %in% ((n.units/2+1):(n.units/2+n.units/(n.periods-1)/2)) ~ 2020,
    id %in% ((n.units/2+n.units/(n.periods-1)/2+1):(n.units/2+n.units*2/(n.periods-1)/2)) ~ 2021,
    id %in% ((n.units/2+n.units*2/(n.periods-1)/2+1):(n.units/2+n.units*3/(n.periods-1)/2)) ~ 2022,
    TRUE ~ 2023
  ), 
         urban = case_when(g==0 ~ sample(c(0,0,1),1),
                           TRUE ~ sample(c(0,1,1),1)) # Urban locations twice as likely to be treated
  ) %>%
  rowwise() %>%
  mutate(treated = case_when(g<=year & g!=0 ~ 1,
                             TRUE ~ 0),
         revenue = 1000000 + #1M intercept plus:
           50000*urban + # additional 50k if in urban area
           100000*treated + # treatment effect: 100k 
           rnorm(n=1, mean =0, sd = 100000) # mean 0 error w/ sd= 100k
         ) 
          
```

Now, to ensure that the reader has a good comprehension of the simulated data set, I provide a few tables to give some insight into the data. The first table shows what the data set looks like for one treated and one untreated store. 

```{r A Glimpse of the Data, echo=TRUE}

## A glimpse of the data ##
#Filter down to the five observations for one treated individual
myData %>% 
  ungroup() %>%
  filter(g==0, urban == 0)%>%
  slice_head(n=5) %>%
  #Stack on top of five obs for one untreated individual
  rbind(
    #Filter down to the five observations for one individual treated in 2017
    myData %>%
      ungroup() %>%
      filter(g==2021, urban==1)%>%
      slice_head(n=5)
  ) %>%
  mutate(revenue = scales::dollar(revenue)) %>%
  #Print as a table
  kable(
    align = "l",
    caption = "Data for One Store That Adopts the Technology and One that Does Not",
    col.names = c("ID","Year","Treatment Group","Urban Dummy","Treated Yet", "Revenue")
  )
```

The table below shows mean revenue by treatment status and whether a store is located in an urban area. 

``` {r Revenue by Treatment and Urban, echo = TRUE}
myData %>%
  group_by(treated, urban) %>%
  summarise(Revenue = mean(revenue) %>% scales::dollar()) %>%
  kable(
    align = "l",
    caption = "Mean Revenue by Treatment Status and Whether Stores are Urban",
    col.names = c("Treated","Urban Dummy", "Mean Revenue")
  )


```

Finally, this table shows the number of units with each treatment group / urban dummy combination.


``` {r Count by Treatment Group and Urban, echo = TRUE}
myData %>%
  group_by(g, urban) %>%
  summarise(Count = n()) %>%
  kable(
    align = "l",
    caption = "Unit-Period Observations in Each Treatment Group by Urban Dummy ",
    col.names = c("Treatment Group", "Urban Dummy", "Count")
  )

```

Now that the housekeeping about the data is out of the way, let's look at how the different estimators perform in this scenario. The {did2s} package makes it very easy for us to compare the results.    

``` {r Estimates For Scenario 1, echo = TRUE}
#Save true effect
true_effect <- data.table(
  term = -4:3,
  true_effect = c(rep(0,4),rep(100000,4))
)

#Run our three estimators
output <- my_event_study(myData, true_effect = true_effect)

#Plot the estimates
plot_event_study(output) + 
  geom_line(aes(x = term, y=true_effect), color = "red")

```
As can be seen above, the three methods all provide similar, unbiased estimates of the effect of the treatment. Note that we have not explicitly controlled for, or conditioned on, the the urban covaraite within any of the three methods. The estimates remain unbiased because although the covariate effects the **level** of the outcome variable, it does not effect the **dynamics** of the outcome variable - it has the same value and same effect in all periods. 

### Heterogeneous Treatment Effects

``` {r Effect Parameters}
#Effect that treatment has in each year
e_20 <- 50000
e_21 <- 100000
e_22 <- 150000
e_23 <- 200000


```

Now, let us consider a case in which the effects of the treatment are heterogeneous. That is, the effects are not the same for all groups and periods. We will make it so that the treatment effect is +`r scales::dollar(e_20)` in revenue in 2020, `r scales::dollar(e_21)` 2021, `r scales::dollar(e_22)` in 2022 and `r scales::dollar(e_23)` in 2023. Perhaps this steady increase in the benefit from the technology arises from some sort of network  effect -  as more people/firms/locations adopt the technology, it becomes more effective. Note that it does not become more effect because units have been in the 'treated' state for more years. It becomes more effective as more calendar years pass, regardless of how long a given unit has been treated.

``` {r Estimates For Scenario 2, echo = TRUE}
#Calculate true effect and save
true_effect <- data.table(
  term = -4:3,
  true_effect = c(rep(0,4),
               (e_20+e_21+e_22+e_23)/4,
               (e_21+e_22+e_23)/3,
               (e_22+e_23)/2,
               e_23)
)

#For reproducibility
set.seed(123)
#Add heterogeneous effects to data
myData2 <- myData %>% 
  mutate(revenue = 1000000 + #1M intercept
           50000*urban +
           #treatment effect dependent on year
           case_when(
             year == 2019 ~ 0,
             year == 2020 ~ treated*e_20,
             year == 2021 ~ treated*e_21,
             year == 2022 ~ treated*e_22,
             year == 2023 ~ treated*e_23
           ) +
           rnorm(n=1, mean =0, sd = 100000) # error w/ mean 0 & sd= 100k
         ) 

#Run our three estimators
output <- my_event_study(myData2, true_effect = true_effect)

#Plot the results
plot_event_study(output) + 
  geom_line(aes(x = term, y=true_effect), color = "red")

```

#### Economic Shock?


### Covariates with Dynamic Effects

Let us return to a situation in which the effects of treatment are not heterogeneous. As in the first simulation, we will say that adopting the technology has the effect of increasing revenues by 100,000 per year in all years. However, we will introduce a covariate that does not have the same effect in all years. Let's imagine that this is a Canadian company with approximately half of its locations in Canada and half in the USA. Initially, in 2019, being located in the USA has a negative effect, perhaps due to inferior brand awareness. In 2019, being located in the USA has the effect of **reducing** revenues by 50 000, all else being equal. However, as time goes on, the brand grows in popularity in the USA, perhaps competitors in the USA start closing down, or the brand just becomes more of a fad in the USA than in Canada. As a result, in 2020, being located in the USA has neither a negative effect nor positive effect, while in 2021 onwards, being located in the United States has the effect of **boosting** revenues by 50 000. Furthermore, Canadian locations are less likely to adopt the technology than American locations, meaning that treatment is correlated with this variable. We will say that American locations are three times as likely to adopt the treatment as Canadian locations. 

Therefore, although the value of this covariate is constant over time, its effect is not constant. Rather than merely affecting the **level** of the outcome variable, this covariate affects the **dynamics** of the outcome variable. First, let us compute the three different estimators that we have been discussing, without explicitly controlling for or conditioning on this new covariate in any of them.  

``` {r Estimates For Scenario 3, echo = TRUE}

#Save true effect
true_effect <- data.table(
  term = -4:3,
  true_effect = c(rep(0,4),rep(100000,4))
)

#For reproducibility
set.seed(123)
#Add the covariate indicating whether location is in Canada
myData <- myData %>%
  group_by(id) %>%
  mutate(usa = case_when(
    g == 0 ~ sample(c(0,0,0,1),1),
    TRUE == 1 ~ sample(c(0,1,1,1),1)
    )
  ) %>%
  rowwise() %>%
  mutate(revenue = 1000000 +
           50000*urban +
           #Effect of canada covariate dependent on year
           case_when(
             year == 2019 ~ usa*-50000,
             year == 2020 ~ usa*0,
             TRUE ~  usa*50000
           ) +
           100000*treated + 
           rnorm(n=1, mean =0, sd = 100000) # error w/ mean 0 & sd= 100k
  ) %>%
  ungroup()

# #View revenue by treatment level, country and year
# inspection <- myData %>%
#   group_by(year, canada, treated) %>%
#   summarise(revenue = mean(revenue))

output <- my_event_study(myData, true_effect = true_effect)

plot_event_study(output) + 
  geom_line(aes(x = term, y=true_effect), color = "red")


# #CS att gt 
# attgt <- att_gt(
#       yname = "revenue",  
#       tname = "year",       
#       idname = "id",        
#       gname = "g",
#       data = myData
# )
# 
# ggdid(attgt) 
#  
```


Alas, all three estimators produce biased estimates for the effect of the technology. In the language of difference-in-differences, this is because the parallel trends assumption does not hold. The 'gap' in revenues between the treatment group and the control group would not have stayed constant in the 'post' periods if the treated group had remained untreated. Because the stores that adopt the technology are disproportionately located in the USA, the treated group would have had lower revenues in 2019, which would then have caught up in 2020 before surpassing those of the untreated group in 2021 and remaining higher in 2022 and 2023. Knowing with certainty that this is the case is the beauty of the Monte Carlo.  

If this were a practical application, the estimates in the 'pre' periods having confidence bands that do not cross zero would give a hint that something is amiss, because this indicates that the pre-trends are not parallel. It is important to note, however, the distinction between parallel **pre**-trends and the parallel trends assumption. Parallel pre-trends are neither sufficient nor necessary for difference-in-differences or TWFE estimates to be valid. The parallel trends assumption, however, is one that the validity of these estimators very much relies on. Testing for parallel pre-trends gives us evidence that the trends between the two groups likely would have remained parallel in the post-training periods had the treated group remained untreated. However, this is not necessarily the case. One could imagine a scenario in which the trends between the two groups were parallel and then something changed post-treatment to effect the dynamics of the outcome variable differently for the treated group than the untreated group. The reverse is also possible. Even if pre-trends are not parallel, it is possible that the outcome paths would have been parallel in the post-period, absent treatment. 

Callaway and Santa'Anna (2020) provide a method for conditioning on covariates that affect outcome dynamics. Specifically, the method is the doubly robust method proposed in Sant'Anna and Zhao (2020). One aspect of this method is to use inverse probability weighting to re-weight the observations in the treatment group according their covariates. Essentially, if a unit in the control group has covariate values that are more prevalent in the treatment group, they will receive a high weight, while an observation with covaraite values similar to most other control group units, it will receive a low weight. 

To my knowledge, in the cases of the both TWFE and Gardner (2021), variables that have constant values within one unit across all periods cannot be controlled for explicitly because of colinearity problems. The coliniearity problems exist because in both cases, additional control variables are included in the same regression as unit-level fixed effects, which would perfectly predict any unit-level variables that do not change over time. The reason that these fixed effects do not result in unbiased estimates in this case, is that they assume that the unit level effects are, well, fixed. Therefore, we will run the Callaway and Sant'Anna estimator with conditioning on the country covariate and inspect the results.   

``` {r Conditioning on Country}

#Estimate CS w/ conditioning on country
output <- event_study(
      yname = "revenue",  
      tname = "year",       
      idname = "id",        
      gname = "g",
      estimator = "did",
      xformla = ~ usa,
      data = myData
      )


#Add column for true effect to DF of estimates
output <- left_join(output, true_effect, by = "term")

plot_event_study(output) + 
  geom_line(aes(x = term, y=true_effect), color = "red")

```
In this case, the covariate conditioning has resulted in unbiased estimates. Does this mean that Callaway and Santa'Anna have created the perfect estimator? Unfortunately note.  

### Covariates with Changing Values

Something that people are often concerned about with difference-in-differences methods such as that of Callaway and Santa'Anna, is that the methods cannot control for variables whose values change over time. In some cases there are ways around this. For example, age changes over time, but we can still condition on it by conditioning on age at a specific date. In other cases, taking the value of a variable at a specific point in time might not work. 

It is important to point out that this is somewhat of a contentious topic. To paraphrase something Pedro Sant'Anna himself said in response to a question about this issue, if a variable changes values in a period after the unit is treated, it is not a covariate, it is an outcome. In many, if not most, cases I think that this is true. If it variable truly is an outcome, or some intermediate step in the the causal path between the treatment and the outcome variable, we certainly do not want to control for it. This would constitute a 'bad control'.  

Speaking within our example, say we wanted to control for the aggregate skill level of the employees at the different locations because with think that might affect revenues and be correlated with technology adoption. If adopting the new technology attracts more educated employees, which in turn increases revenues, that increase in revenues is a result of the technology adoption. Controlling for education would 'wipe out' this effect from out estimates. Simulating results for bad controls is beyond the scope of this document, but perhaps I will do a write up about them in the future.

As a counter-example, imagine that there is some sort of new policy that is being rolled out by some municipalities but not others. The policy hurts revenues at locations in these municipalities (maybe it is some sort of a 'sin' tax on our products, or a ban on a particularly effective form of marketing).Moreover, municipalities in Canada are twice as likely to introduce this policy as American municipalities. Recall that Canadian municipalities are less likely to adopt the technology, which means that this new policy variable is negatively correlated with treatment. Last, let's assume that the stores are all (or nearly all) located in different munucipalities. If municipalities were sufficiently large and locations were sufficiently plentiful that there were treated and untreated units in every municipality, one could actually arrive at an unbiased estimate by conditioning on municipality using Callaway and Sant'Anna (2020). 

In this simulation revenue is determined as follows. For each store, revenue in each year is equal to one million, minus 200 000 if a location is in a municipality with the policy discussed above, plus the 100 000 treatment effect, plus an idiosyncratic error term which is drawn from random normal distribution with mean zero and a standard deviation of 100 000. The policy is introduced randomly in any of the five years or not at all. MUST RE WORD In any given year for a Canadian location, the odds of the policy being implemented by the local municipality are 1 in 10. For locations in the USA, the odds are 1 in 20.    

```{r Covariates With Changing Values}

#For reproducibility
set.seed(123)

#Add our policy variable and calculate our new revenue
myData <-  myData %>%
  group_by(id) %>%
  mutate(policy_year = case_when(
                                 usa == 1 ~ sample(c(2019,2020,2021,2022,2023, rep(0,15)),1),
                                 usa == 0 ~ sample(c(2019,2020,2021,2022,2023, rep(0,5)),1)
                                 )
         )%>%
  rowwise() %>%
  mutate(policy = case_when(policy_year<=year & policy_year !=0 ~ 1,
                            TRUE ~ 0),
         revenue = 1000000
                   + policy * -200000
                   + treated * 100000
                   + rnorm(n=1, mean =0, sd = 100000) # error w/ mean 0 & sd= 100k
  )

output <- my_event_study(myData, true_effect = true_effect)

plot_event_study(output) + 
  geom_line(aes(x = term, y=true_effect), color = "red")

#Estimate model with conditioning on policy variable
output <- my_event_study(myData, xformla = ~policy, true_effect = true_effect)

plot_event_study(output) + 
  geom_line(aes(x = term, y=true_effect), color = "red")




```

### Treatment that 'Switches Off'








